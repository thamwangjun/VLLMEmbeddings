version: "3.8"

services:
  vllm:
    shm_size: 16gb
    image: serve-nm-vllm/nm-vllm:v0.2.0.aqlm1
    build:
      context: ./nm-vllm
      dockerfile: Dockerfile
    command: --model $LLM_HF_REPO_ID $VLLM_ARGS
    volumes:
      - ./workspace:/vllm-workspace
    networks:
      - vllmnet
    restart: always
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  text-embeddings-inference:
    image: serve-nm-vllm/text-embeddings-inference:1.2
    build:
      context: .
      dockerfile: Dockerfile.text_embeddings_inference
    command: --model-id $EMBEDDINGS_HF_REPO_ID $TEXT_EMBEDDINGS_INFERENCE_ARGS
    volumes:
      - ./embeddings-workspace:/data
    networks:
      - vllmnet
    restart: always
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  nginx:
    image: serve-nm-vllm/nginx:1.25.0
    build:
      context: .
      dockerfile: Dockerfile.nginx
    networks:
      - vllmnet
    ports:
      - ${API_PORT:-8080}:8080
networks:
  - vllmnet: